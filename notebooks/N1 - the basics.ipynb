{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b816f18",
   "metadata": {},
   "source": [
    "As a preparation for the new material about artificial neural networks, we review basic concepts covered in term 1. \n",
    "\n",
    "# basic data structures\n",
    "\n",
    "Python has several built-in data structures that you can use to store and organize data. These data structures include lists, tuples, sets, and dictionaries.\n",
    "\n",
    "Lists are ordered collections of items. They are mutable, which means that you can change the items in a list after it is created. You can create a list by enclosing a comma-separated sequence of items in square brackets []. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8d85e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = [1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69defa2c",
   "metadata": {},
   "source": [
    "Tuples are also ordered collections of items, but they are immutable, which means that you cannot modify the items in a tuple after it is created. You can create a tuple by enclosing a comma-separated sequence of items in parentheses (). For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "224473ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tuple = (1, 2, 3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28f1ff0",
   "metadata": {},
   "source": [
    "Sets are unordered collections of unique items. They are mutable and are created using the set keyword or by enclosing a comma-separated sequence of items in curly braces {}. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc6a806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_set = {1, 2, 3, 4}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb1780d",
   "metadata": {},
   "source": [
    "Dictionaries are unordered collections of key-value pairs. They are mutable and are created using the dict keyword or by enclosing a comma-separated sequence of key-value pairs in curly braces {}. The key and value are separated by a colon :. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e004648",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {'a': 1, 'b': 2, 'c': 3, 'd': 4}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd780236",
   "metadata": {},
   "source": [
    "There are several ways to store a matrix in Python. One way is to use a list of lists, where each inner list represents a row of the matrix. For example, here's how you could create a 2x2 matrix using a list of lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a15af83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2], [3, 4]]\n"
     ]
    }
   ],
   "source": [
    "matrix = [[1, 2], [3, 4]]\n",
    "matrix[1][0]  # Output: 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c96e52",
   "metadata": {},
   "source": [
    "Another option is to use a NumPy array, which is a multidimensional array object provided by the NumPy library. NumPy arrays are more efficient for storing and manipulating large matrices. Here's how you could create a 2x2 matrix using a NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00a14670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "matrix = np.array([[1, 2], [3, 4]])\n",
    "matrix[1, 0]  # Output: 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815e7e4a",
   "metadata": {},
   "source": [
    "A tensor is a multi-dimensional array of data, therefore can be created using the np.array method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6baff587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = np.array([[[1,2],[3,4]],[[5,6],[7,8]]])\n",
    "tensor[1,0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f620c1",
   "metadata": {},
   "source": [
    "# computation\n",
    "\n",
    "In Python, you can perform matrix multiplication using the dot function from the NumPy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ec36b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19 22]\n",
      " [43 50]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "matrix_a = np.array([[1, 2], [3, 4]])\n",
    "matrix_b = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "result = np.dot(matrix_a, matrix_b)\n",
    "\n",
    "print(result)  # Output: [[19 22], [43 50]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aed53b",
   "metadata": {},
   "source": [
    "In NumPy, you can perform element-wise multiplication of two arrays using the * operator or the multiply function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd4daa34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4 10 18]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "array_a = np.array([1, 2, 3])\n",
    "array_b = np.array([4, 5, 6])\n",
    "\n",
    "result = array_a * array_b\n",
    "print(result)  # Output: [4 10 18]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b44a88",
   "metadata": {},
   "source": [
    "You can also use the multiply function to perform element-wise multiplication of two arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23d0dfe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4 10 18]\n"
     ]
    }
   ],
   "source": [
    "result = np.multiply(array_a, array_b)\n",
    "print(result)  # Output: [4 10 18]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d4dc0b",
   "metadata": {},
   "source": [
    "# non-linear activation functions\n",
    "\n",
    "The sigmoid function is a non-linear activation function that maps its input to values between 0 and 1.\n",
    "\n",
    "The ReLU function is another commonly used activation function that maps negative input values to 0 and leaves positive input values unchanged.\n",
    "\n",
    "The softmax function is a generalization of the logistic function that maps a K-dimensional vector of real values to a K-dimensional vector of values between 0 and 1 that sum to 1. It is often used as an activation function in the output layer of a multi-class classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a101d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    # Subtract the maximum value from each element to prevent overflow\n",
    "    x = x - np.max(x)\n",
    "    # Calculate the exponent of each element\n",
    "    exp_x = np.exp(x)\n",
    "    # Return the normalized exponents\n",
    "    return exp_x / np.sum(exp_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "031fe94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([-1, 2, 3])\n",
    "y = sigmoid(x)\n",
    "z = relu(x)\n",
    "w = softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2f295cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.26894142, 0.88079708, 0.95257413]),\n",
       " array([0, 2, 3]),\n",
       " array([0.01321289, 0.26538793, 0.72139918]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, z, w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c4d8c1",
   "metadata": {},
   "source": [
    "# loss functions\n",
    "\n",
    "Cross entropy loss is a common loss function used in supervised learning problems, particularly in classification tasks. It measures the difference between the predicted probability distribution and the true probability distribution of the target classes.\n",
    "\n",
    "Here's how you can define the cross entropy loss function in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "520fb30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2039728043259361\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    # Clip the predicted probabilities to prevent log(0) errors\n",
    "    y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "    # Calculate the cross entropy loss\n",
    "    loss = -np.sum(y_true * np.log(y_pred))\n",
    "    return loss\n",
    "\n",
    "y_pred = np.array([[0.1, 0.3, 0.6]])\n",
    "y_true = np.array([[0, 1, 0]])\n",
    "\n",
    "loss = cross_entropy_loss(y_pred, y_true)\n",
    "print(loss)  # Output: 1.2039728043259361"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c998ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0.1,0.5,0.4])\n",
    "y = np.array([[0.1,0.5,0.4]])\n",
    "z = np.array([[[0.1,0.5,0.4]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fe497f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3,), (1, 3), (1, 1, 3))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape, z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df46b43d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd4fb08",
   "metadata": {},
   "source": [
    "# gradient descent\n",
    "\n",
    "Gradient descent is an optimization algorithm that is used to find the values of parameters (coefficients) of a function (f) that minimizes a cost function (J). The algorithm works by iteratively updating the values of the parameters in the opposite direction of the gradient of the cost function with respect to the parameters.\n",
    "\n",
    "Here's a simple implementation of the gradient descent algorithm in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e838fd25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.42724769e-05, 7.13623846e-05])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent(f, x0, lr=0.1, num_iter=100):\n",
    "    x = x0.copy()\n",
    "    for i in range(num_iter):\n",
    "        # Calculate the gradient of the cost function\n",
    "        grad = gradient(f, x)\n",
    "        # Update the parameters\n",
    "        x -= lr * grad\n",
    "    return x\n",
    "\n",
    "def gradient(f, x):\n",
    "    # Set a small value for h\n",
    "    h = 1e-7\n",
    "    # Calculate the gradient\n",
    "    grad = np.zeros_like(x)\n",
    "    for i in range(x.size):\n",
    "        # Save the current value of x[i]\n",
    "        tmp = x[i]\n",
    "        # Calculate f(x + h)\n",
    "        x[i] = tmp + h\n",
    "        fxh1 = f(x)\n",
    "        # Calculate f(x - h)\n",
    "        x[i] = tmp - h\n",
    "        fxh2 = f(x)\n",
    "        # Restore the value of x[i]\n",
    "        x[i] = tmp\n",
    "        # Approximate the gradient\n",
    "        grad[i] = (fxh1 - fxh2) / (2 * h)\n",
    "    return grad\n",
    "\n",
    "def f(x):\n",
    "    return np.sum(x*x)\n",
    "\n",
    "x0 = np.array([1.0, 5.0])\n",
    "gradient_descent(f,x0,lr=0.1,num_iter=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0aa3b5",
   "metadata": {},
   "source": [
    "you can use PyTorch to perform automatic differentiation. PyTorch is a popular deep learning library that provides automatic differentiation through a system called autograd.\n",
    "\n",
    "To use autograd in PyTorch, you need to define a torch.Tensor object and set its requires_grad attribute to True. This tells PyTorch to keep track of the operations performed on the tensor and compute the gradient when necessary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b209ea",
   "metadata": {},
   "source": [
    "Here's an example of how you can use automatic differentiation to implement the gradient descent algorithm in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1380a633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3600277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.03703598e-10 4.07407195e-10]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Set the device to use for computations\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def gradient_descent(f, x0, lr=0.1, num_iter=100):\n",
    "    # Convert x0 to a tensor with requires_grad=True\n",
    "    x = torch.tensor(x0, requires_grad=True, device=device)\n",
    "    for i in range(num_iter):\n",
    "        # Calculate the gradient of the cost function\n",
    "        y = f(x)\n",
    "        y.backward()\n",
    "        # Update the parameters\n",
    "        with torch.no_grad():\n",
    "            x -= lr * x.grad\n",
    "        # Zero the gradients\n",
    "        x.grad.zero_()\n",
    "    return x.detach().cpu().numpy()\n",
    "\n",
    "# Define the cost function\n",
    "def cost_function(x):\n",
    "    return torch.sum(x*x)\n",
    "\n",
    "# Set the initial values of the parameters\n",
    "x0 = np.array([1, 2],dtype=float)\n",
    "\n",
    "# Set the learning rate\n",
    "lr = 0.1\n",
    "\n",
    "# Set the number of iterations\n",
    "num_iter = 100\n",
    "\n",
    "# Find the optimal values of the parameters\n",
    "x = gradient_descent(cost_function, x0, lr, num_iter)\n",
    "\n",
    "print(x) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447923b7",
   "metadata": {},
   "source": [
    "## remark 1 \n",
    "\n",
    "In PyTorch, the torch.no_grad context manager is used to temporarily disable gradient calculation. When gradient calculation is disabled, PyTorch will not track the operations performed within the context, and the gradients of tensors will not be updated.\n",
    "\n",
    "This can be useful in situations where you want to perform an operation that does not require gradient calculation, such as updating the parameters of a model. By using torch.no_grad, you can avoid unnecessary overhead and improve the performance of your code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4d7625",
   "metadata": {},
   "source": [
    "## remark 2\n",
    "\n",
    "the gradient_descent function returns the result of calling the detach, cpu, and numpy methods on the input tensor x.\n",
    "\n",
    "The detach method is used to create a new tensor that does not require gradient calculation. This is useful when you want to return a tensor from a PyTorch function and use it outside of a PyTorch context, as the tensor will not retain a reference to the computation graph and will not consume additional memory.\n",
    "\n",
    "The cpu method is used to move the tensor from the GPU (if it is on the GPU) to the CPU. This can be useful if you want to use the tensor with a NumPy function or save it to a file.\n",
    "\n",
    "The numpy method is used to convert the tensor to a NumPy array. This can be useful if you want to use the tensor with other Python libraries that do not support PyTorch tensors.\n",
    "\n",
    "Alternatively, you could return the tensor x directly without calling these methods. However, the tensor would still be a PyTorch tensor that requires gradient calculation and is stored on the GPU (if applicable), which might not be what you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e6df8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
