{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de3fd25b",
   "metadata": {},
   "source": [
    "To load the MNIST data with PyTorch, you can use the torchvision package, which provides a set of utilities for loading and transforming datasets, including the MNIST dataset. Here's an example of how you can load the MNIST training data with torchvision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0f1221d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7.9%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Transform the data to tensors and normalize it\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load the training data\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "# Test Data\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14260beb",
   "metadata": {},
   "source": [
    "To view the first examples in the MNIST dataset after loading it with PyTorch, you can use the iter function to retrieve the first mini-batch of examples from the data loader, and then use the `.numpy()` method to convert the tensors to numpy arrays, which can be easily plotted using Matplotlib or a similar library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89c8d049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAcHUlEQVR4nO3dfWyV9f3/8dcpN0eUnsNK7R0ULKgwxWKG0HUoojSUbiOAzLuZCIvRwIqKeJcaFZ0udWxxzIXh/lhgbuINi0AkSoLFljkLBpARMu1o040aaJlEzoFiS0M/vz/4eb4caYHP4bTvtjwfySfpua7rfa43H66cV69zrl4n4JxzAgCgm6VYNwAAuDgRQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADDR37qBb2tvb9eBAweUmpqqQCBg3Q4AwJNzTkePHlVOTo5SUjo/z+lxAXTgwAHl5uZatwEAuEANDQ0aPnx4p+t73Ftwqamp1i0AAJLgXK/nXRZAK1as0BVXXKFLLrlEBQUF+uSTT86rjrfdAKBvONfreZcE0FtvvaUlS5Zo6dKl2rVrl8aPH6/i4mIdOnSoK3YHAOiNXBeYNGmSKy0tjT0+efKky8nJceXl5eesjUQiThKDwWAwevmIRCJnfb1P+hnQiRMntHPnThUVFcWWpaSkqKioSNXV1Wds39raqmg0GjcAAH1f0gPoyy+/1MmTJ5WZmRm3PDMzU42NjWdsX15ernA4HBtcAQcAFwfzq+DKysoUiURio6GhwbolAEA3SPrfAaWnp6tfv35qamqKW97U1KSsrKwztg8GgwoGg8luAwDQwyX9DGjgwIGaMGGCKioqYsva29tVUVGhwsLCZO8OANBLdcmdEJYsWaJ58+bphhtu0KRJk7R8+XI1NzfrZz/7WVfsDgDQC3VJAN1555363//+p2effVaNjY26/vrrtWnTpjMuTAAAXLwCzjln3cTpotGowuGwdRsAgAsUiUQUCoU6XW9+FRwA4OJEAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATPS3bgAXl2HDhnnX/OY3v/GuueOOO7xrElVRUeFds3HjRu+alStXete0tbV51wDdhTMgAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJrgZKRKWyI1FX3rpJe+a22+/3bvGOeddk6hbb721W2qGDx/uXfPEE0941wDdhTMgAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgKuO+/aeB6i0ajC4bB1GzgPW7Zs8a6ZOHGid82aNWu8a95++23vGklqamryrvnJT37iXfP000971yTi+uuvT6hu7969yW0EF6VIJKJQKNTpes6AAAAmCCAAgImkB9Bzzz2nQCAQN8aOHZvs3QAAerku+UK6a6+9Vh988MH/7aQ/33sHAIjXJcnQv39/ZWVldcVTAwD6iC75DGjfvn3KycnRqFGjdM8992j//v2dbtva2qpoNBo3AAB9X9IDqKCgQKtXr9amTZu0cuVK1dfX66abbtLRo0c73L68vFzhcDg2cnNzk90SAKAHSnoAlZSU6Pbbb1d+fr6Ki4v13nvv6ciRI53+XUZZWZkikUhsNDQ0JLslAEAP1OVXBwwZMkRXX321amtrO1wfDAYVDAa7ug0AQA/T5X8HdOzYMdXV1Sk7O7urdwUA6EWSHkCPPfaYqqqq9J///Ecff/yx5syZo379+unuu+9O9q4AAL1Y0t+C++KLL3T33Xfr8OHDuvzyy3XjjTdq27Ztuvzyy5O9KwBAL8bNSJGwQYMGedcEAgHvmuPHj3vXdKeUFP83Ep5//nnvmrKyMu+aw4cPe9dI0s033+xd8/nnnye0L/Rd3IwUANAjEUAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMNHlX0iHvuvrr7+2bqFHaG9v96559dVXvWt+/OMfe9fk5+d710jSTTfd5F3DzUjhizMgAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAICJgHPOWTdxumg0qnA4bN0G0OM8/PDD3jUvv/xyQvuqqKjwrpk+fXpC+0LfFYlEFAqFOl3PGRAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAAT/a0bANDz9OvXz7smJcX/99n29nbvGvQdnAEBAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwwc1IgV7igw8+6LZ93Xzzzd4111xzjXfN3r17vWvQd3AGBAAwQQABAEx4B9DWrVs1c+ZM5eTkKBAIaP369XHrnXN69tlnlZ2drUGDBqmoqEj79u1LVr8AgD7CO4Cam5s1fvx4rVixosP1y5Yt0yuvvKJXX31V27dv12WXXabi4mK1tLRccLMAgL7D+yKEkpISlZSUdLjOOafly5fr6aef1qxZsyRJr732mjIzM7V+/XrdddddF9YtAKDPSOpnQPX19WpsbFRRUVFsWTgcVkFBgaqrqzusaW1tVTQajRsAgL4vqQHU2NgoScrMzIxbnpmZGVv3beXl5QqHw7GRm5ubzJYAAD2U+VVwZWVlikQisdHQ0GDdEgCgGyQ1gLKysiRJTU1Nccubmppi674tGAwqFArFDQBA35fUAMrLy1NWVpYqKipiy6LRqLZv367CwsJk7goA0Mt5XwV37Ngx1dbWxh7X19dr9+7dSktL04gRI7R48WK9+OKLuuqqq5SXl6dnnnlGOTk5mj17djL7BgD0ct4BtGPHDt1yyy2xx0uWLJEkzZs3T6tXr9YTTzyh5uZmPfDAAzpy5IhuvPFGbdq0SZdccknyugYA9HoB55yzbuJ00WhU4XDYug2gx5kzZ453zd/+9reE9rVr1y7vmtN/MT1fx44d865B7xGJRM76ub75VXAAgIsTAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMCE99cxALhwRUVF3jULFy7sgk469tVXX3nXcGdr+OIMCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAluRgoYqK6u9q4ZM2aMd01KSmK/Y/79739PqA7wwRkQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAE9yMFDjNjTfe6F0TDAa9axYsWOBdM2zYMO+a9vZ275pE6wYPHuxdc+zYMe8a9B2cAQEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADARcM456yZOF41GFQ6HrdtAD5KRkeFd8+ijjya0r8WLF3vX9OvXL6F9dYdAIJBQXSIvCxs2bPCueeedd7xrXn/9de8a2IhEIgqFQp2u5wwIAGCCAAIAmPAOoK1bt2rmzJnKyclRIBDQ+vXr49bPnz9fgUAgbsyYMSNZ/QIA+gjvAGpubtb48eO1YsWKTreZMWOGDh48GBtvvPHGBTUJAOh7vL8RtaSkRCUlJWfdJhgMKisrK+GmAAB9X5d8BlRZWamMjAyNGTNGCxcu1OHDhzvdtrW1VdFoNG4AAPq+pAfQjBkz9Nprr6miokK/+tWvVFVVpZKSEp08ebLD7cvLyxUOh2MjNzc32S0BAHog77fgzuWuu+6K/XzdddcpPz9fo0ePVmVlpaZNm3bG9mVlZVqyZEnscTQaJYQA4CLQ5Zdhjxo1Sunp6aqtre1wfTAYVCgUihsAgL6vywPoiy++0OHDh5Wdnd3VuwIA9CLeb8EdO3Ys7mymvr5eu3fvVlpamtLS0vT8889r7ty5ysrKUl1dnZ544gldeeWVKi4uTmrjAIDezTuAduzYoVtuuSX2+JvPb+bNm6eVK1dqz549+vOf/6wjR44oJydH06dP1wsvvKBgMJi8rgEAvR43I0W3Gjx4sHfNgw8+6F3zwgsveNdI0vvvv+9d88knn3jX3Hfffd41w4cP96557733vGskaejQod41N9xwg3dNSor/pwD19fXeNR1dAHU+GhoaEqrDKdyMFADQIxFAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATCT9K7mBs1m+fLl3zfz5871r3nnnHe8aSbr33nu9axK5W3cid7ZOxOOPP55QXU1NjXfNY4895l3z0EMPedeMGjXKuyaR3hKta2trS2hfFyPOgAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgIOOecdROni0ajCofD1m3gPMyaNcu7Zu3atd41n332mXfN5MmTvWskaeTIkd41v/vd77xrpk6d6l3z73//27vmmmuu8a7pTvn5+d41u3bt6oJOOpbI/9NHH32U/EZ6qUgkolAo1Ol6zoAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCY6G/dAHqvsrIy75qUFP/feZYvX+5d09LS4l0jSRs3bvSuyc3N9a7Zt2+fd01RUZF3TU+3d+9e75qnnnrKu+bFF1/0rpGk4uJi7xpuRnr+OAMCAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgIuCcc9ZNnC4ajSocDlu3gfPwz3/+07vm2muv9a6prKz0rknU1KlTvWv27NnjXfPkk09612zevNm7Bqd8/PHH3bavH/zgB922r54uEokoFAp1up4zIACACQIIAGDCK4DKy8s1ceJEpaamKiMjQ7Nnz1ZNTU3cNi0tLSotLdXQoUM1ePBgzZ07V01NTUltGgDQ+3kFUFVVlUpLS7Vt2zZt3rxZbW1tmj59upqbm2PbPPLII3r33Xe1du1aVVVV6cCBA7rtttuS3jgAoHfz+kbUTZs2xT1evXq1MjIytHPnTk2ZMkWRSER/+tOftGbNGt16662SpFWrVum73/2utm3bpu9///vJ6xwA0Ktd0GdAkUhEkpSWliZJ2rlzp9ra2uK+Onjs2LEaMWKEqqurO3yO1tZWRaPRuAEA6PsSDqD29nYtXrxYkydP1rhx4yRJjY2NGjhwoIYMGRK3bWZmphobGzt8nvLycoXD4djIzc1NtCUAQC+ScACVlpZq7969evPNNy+ogbKyMkUikdhoaGi4oOcDAPQOXp8BfWPRokXauHGjtm7dquHDh8eWZ2Vl6cSJEzpy5EjcWVBTU5OysrI6fK5gMKhgMJhIGwCAXszrDMg5p0WLFmndunXasmWL8vLy4tZPmDBBAwYMUEVFRWxZTU2N9u/fr8LCwuR0DADoE7zOgEpLS7VmzRpt2LBBqampsc91wuGwBg0apHA4rPvuu09LlixRWlqaQqGQHnzwQRUWFnIFHAAgjlcArVy5UtKZ98tatWqV5s+fL0n67W9/q5SUFM2dO1etra0qLi7WH/7wh6Q0CwDoO7gZKRK2bt0675qZM2d2QSfJs2/fPu+ahx56yLuGG4t2r7/85S8J1d1xxx3eNbNnz/auef/9971regNuRgoA6JEIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYS+kZUQJLuvfde75r33nvPu+aKK67wrvnlL3/pXSNJb731lnfNV199ldC+0H0WLlyYUF1dXZ13zUcffZTQvi5GnAEBAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwEXDOOesmTheNRhUOh63bAABcoEgkolAo1Ol6zoAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmPAKoPLyck2cOFGpqanKyMjQ7NmzVVNTE7fN1KlTFQgE4saCBQuS2jQAoPfzCqCqqiqVlpZq27Zt2rx5s9ra2jR9+nQ1NzfHbXf//ffr4MGDsbFs2bKkNg0A6P36+2y8adOmuMerV69WRkaGdu7cqSlTpsSWX3rppcrKykpOhwCAPumCPgOKRCKSpLS0tLjlr7/+utLT0zVu3DiVlZXp+PHjnT5Ha2urotFo3AAAXARcgk6ePOl+9KMfucmTJ8ct/+Mf/+g2bdrk9uzZ4/7617+6YcOGuTlz5nT6PEuXLnWSGAwGg9HHRiQSOWuOJBxACxYscCNHjnQNDQ1n3a6iosJJcrW1tR2ub2lpcZFIJDYaGhrMJ43BYDAYFz7OFUBenwF9Y9GiRdq4caO2bt2q4cOHn3XbgoICSVJtba1Gjx59xvpgMKhgMJhIGwCAXswrgJxzevDBB7Vu3TpVVlYqLy/vnDW7d++WJGVnZyfUIACgb/IKoNLSUq1Zs0YbNmxQamqqGhsbJUnhcFiDBg1SXV2d1qxZox/+8IcaOnSo9uzZo0ceeURTpkxRfn5+l/wDAAC9lM/nPurkfb5Vq1Y555zbv3+/mzJliktLS3PBYNBdeeWV7vHHHz/n+4Cni0Qi5u9bMhgMBuPCx7le+wP/P1h6jGg0qnA4bN0GAOACRSIRhUKhTtdzLzgAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgIkeF0DOOesWAABJcK7X8x4XQEePHrVuAQCQBOd6PQ+4HnbK0d7ergMHDig1NVWBQCBuXTQaVW5urhoaGhQKhYw6tMc8nMI8nMI8nMI8nNIT5sE5p6NHjyonJ0cpKZ2f5/Tvxp7OS0pKioYPH37WbUKh0EV9gH2DeTiFeTiFeTiFeTjFeh7C4fA5t+lxb8EBAC4OBBAAwESvCqBgMKilS5cqGAxat2KKeTiFeTiFeTiFeTilN81Dj7sIAQBwcehVZ0AAgL6DAAIAmCCAAAAmCCAAgIleE0ArVqzQFVdcoUsuuUQFBQX65JNPrFvqds8995wCgUDcGDt2rHVbXW7r1q2aOXOmcnJyFAgEtH79+rj1zjk9++yzys7O1qBBg1RUVKR9+/bZNNuFzjUP8+fPP+P4mDFjhk2zXaS8vFwTJ05UamqqMjIyNHv2bNXU1MRt09LSotLSUg0dOlSDBw/W3Llz1dTUZNRx1zifeZg6deoZx8OCBQuMOu5Yrwigt956S0uWLNHSpUu1a9cujR8/XsXFxTp06JB1a93u2muv1cGDB2Pjo48+sm6pyzU3N2v8+PFasWJFh+uXLVumV155Ra+++qq2b9+uyy67TMXFxWppaenmTrvWueZBkmbMmBF3fLzxxhvd2GHXq6qqUmlpqbZt26bNmzerra1N06dPV3Nzc2ybRx55RO+++67Wrl2rqqoqHThwQLfddpth18l3PvMgSffff3/c8bBs2TKjjjvheoFJkya50tLS2OOTJ0+6nJwcV15ebthV91u6dKkbP368dRumJLl169bFHre3t7usrCz361//OrbsyJEjLhgMujfeeMOgw+7x7Xlwzrl58+a5WbNmmfRj5dChQ06Sq6qqcs6d+r8fMGCAW7t2bWybzz77zEly1dXVVm12uW/Pg3PO3Xzzze7hhx+2a+o89PgzoBMnTmjnzp0qKiqKLUtJSVFRUZGqq6sNO7Oxb98+5eTkaNSoUbrnnnu0f/9+65ZM1dfXq7GxMe74CIfDKigouCiPj8rKSmVkZGjMmDFauHChDh8+bN1Sl4pEIpKktLQ0SdLOnTvV1tYWdzyMHTtWI0aM6NPHw7fn4Ruvv/660tPTNW7cOJWVlen48eMW7XWqx92M9Nu+/PJLnTx5UpmZmXHLMzMz9fnnnxt1ZaOgoECrV6/WmDFjdPDgQT3//PO66aabtHfvXqWmplq3Z6KxsVGSOjw+vll3sZgxY4Zuu+025eXlqa6uTk899ZRKSkpUXV2tfv36WbeXdO3t7Vq8eLEmT56scePGSTp1PAwcOFBDhgyJ27YvHw8dzYMk/fSnP9XIkSOVk5OjPXv26Mknn1RNTY3eeecdw27j9fgAwv8pKSmJ/Zyfn6+CggKNHDlSb7/9tu677z7DztAT3HXXXbGfr7vuOuXn52v06NGqrKzUtGnTDDvrGqWlpdq7d+9F8Tno2XQ2Dw888EDs5+uuu07Z2dmaNm2a6urqNHr06O5us0M9/i249PR09evX74yrWJqampSVlWXUVc8wZMgQXX311aqtrbVuxcw3xwDHx5lGjRql9PT0Pnl8LFq0SBs3btSHH34Y9/UtWVlZOnHihI4cORK3fV89Hjqbh44UFBRIUo86Hnp8AA0cOFATJkxQRUVFbFl7e7sqKipUWFho2Jm9Y8eOqa6uTtnZ2datmMnLy1NWVlbc8RGNRrV9+/aL/vj44osvdPjw4T51fDjntGjRIq1bt05btmxRXl5e3PoJEyZowIABccdDTU2N9u/f36eOh3PNQ0d2794tST3reLC+CuJ8vPnmmy4YDLrVq1e7f/3rX+6BBx5wQ4YMcY2NjdatdatHH33UVVZWuvr6evePf/zDFRUVufT0dHfo0CHr1rrU0aNH3aeffuo+/fRTJ8m9/PLL7tNPP3X//e9/nXPOvfTSS27IkCFuw4YNbs+ePW7WrFkuLy/Pff3118adJ9fZ5uHo0aPusccec9XV1a6+vt598MEH7nvf+5676qqrXEtLi3XrSbNw4UIXDoddZWWlO3jwYGwcP348ts2CBQvciBEj3JYtW9yOHTtcYWGhKywsNOw6+c41D7W1te4Xv/iF27Fjh6uvr3cbNmxwo0aNclOmTDHuPF6vCCDnnPv973/vRowY4QYOHOgmTZrktm3bZt1St7vzzjtddna2GzhwoBs2bJi78847XW1trXVbXe7DDz90ks4Y8+bNc86duhT7mWeecZmZmS4YDLpp06a5mpoa26a7wNnm4fjx42769Onu8ssvdwMGDHAjR450999/f5/7Ja2jf78kt2rVqtg2X3/9tfv5z3/uvvOd77hLL73UzZkzxx08eNCu6S5wrnnYv3+/mzJliktLS3PBYNBdeeWV7vHHH3eRSMS28W/h6xgAACZ6/GdAAIC+iQACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgIn/B+fN8eRgy0sBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the first batch of training examples\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "# Convert the images to numpy arrays and plot them\n",
    "plt.imshow(images[0].numpy().squeeze(), cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544d0198",
   "metadata": {},
   "source": [
    "The .squeeze() method is used to remove single-dimensional entries from the shape of an array. In the case of image data, this is often used to remove the singleton dimensions from the shape of the tensor that represent the number of color channels.\n",
    "\n",
    "For example, if you have an image tensor with shape (1, 28, 28), which represents a single grayscale image with height 28 and width 28, you can use .squeeze() to remove the leading dimension with size 1, resulting in a tensor with shape (28, 28):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c878dd33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor = torch.tensor([[[1.0, 2.0, 3.0],\n",
    "                              [4.0, 5.0, 6.0],\n",
    "                              [7.0, 8.0, 9.0]]])\n",
    "\n",
    "image_tensor.shape  # (1, 3, 3)\n",
    "image_tensor.squeeze().shape  # (3, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "924c069a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "567598a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.3398\n",
      "Epoch [1/5], Step [200/600], Loss: 0.2516\n",
      "Epoch [1/5], Step [300/600], Loss: 0.2974\n",
      "Epoch [1/5], Step [400/600], Loss: 0.2057\n",
      "Epoch [1/5], Step [500/600], Loss: 0.2116\n",
      "Epoch [1/5], Step [600/600], Loss: 0.2058\n",
      "Epoch [2/5], Step [100/600], Loss: 0.1313\n",
      "Epoch [2/5], Step [200/600], Loss: 0.1723\n",
      "Epoch [2/5], Step [300/600], Loss: 0.1451\n",
      "Epoch [2/5], Step [400/600], Loss: 0.1027\n",
      "Epoch [2/5], Step [500/600], Loss: 0.0419\n",
      "Epoch [2/5], Step [600/600], Loss: 0.1451\n",
      "Epoch [3/5], Step [100/600], Loss: 0.1168\n",
      "Epoch [3/5], Step [200/600], Loss: 0.0360\n",
      "Epoch [3/5], Step [300/600], Loss: 0.1214\n",
      "Epoch [3/5], Step [400/600], Loss: 0.0231\n",
      "Epoch [3/5], Step [500/600], Loss: 0.0956\n",
      "Epoch [3/5], Step [600/600], Loss: 0.0470\n",
      "Epoch [4/5], Step [100/600], Loss: 0.0342\n",
      "Epoch [4/5], Step [200/600], Loss: 0.0475\n",
      "Epoch [4/5], Step [300/600], Loss: 0.0354\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0310\n",
      "Epoch [4/5], Step [500/600], Loss: 0.0798\n",
      "Epoch [4/5], Step [600/600], Loss: 0.1203\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0235\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0569\n",
      "Epoch [5/5], Step [300/600], Loss: 0.0379\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0114\n",
      "Epoch [5/5], Step [500/600], Loss: 0.0829\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0244\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data',\n",
    "                                           train=True,\n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=False)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data',\n",
    "                                          train=False,\n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=100,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=100,\n",
    "                                          shuffle=False)\n",
    "\n",
    "# Neural network\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = NeuralNet(input_size=28*28, hidden_size=500, num_classes=10).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Move tensors to the configured device\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a088bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 97.78 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "\n",
    "# Iterate through the test data\n",
    "for images, labels in test_loader:\n",
    "    # Move the data to the device where the model is running\n",
    "    images = images.reshape(-1, 28*28).to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(images)\n",
    "\n",
    "    # Get the predictions\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "    # Update the accuracy\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Print the accuracy\n",
    "print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf6ad97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
